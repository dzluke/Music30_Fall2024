{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 6: Sound-types\n",
    "\n",
    "As we saw in class, sound-types is a multi-layer framework for representing and synthesizing sounds. In this lab, we will apply the theory of sound-types for creating new audio from input audio. The two main types of synthesis that are possible through the sound-types are *probabilistic generation* and *sound hybridization*. Sections 1 and 2 will walk you through using the sound-types to apply probabilistic generation to an input sound. In Section 3, you will combine two input sounds through the process of sound hybridization.\n",
    "\n",
    "First, let's briefly review the theory of sound-types. The analysis phase of sound-types takes in an input sample as audio, and performs the following steps:\n",
    "1. **atomize**: the input sound is divided into very small (40 ms) overlapping chunks called *atoms*\n",
    "2. **make classes**: for each atom, compute low-level descriptors that allow you to represent the atoms in a feature space. Then, we can use this space to see whcih atoms are closer to each other (and therefore more similar), and creater groups (clusters) of similar atoms.\n",
    "3. **compute probabilities**: finally, determine the sequential relationship between the clusters of atoms computed in the step 2. Using a Markov chain, we can estimate the probabilities that one cluster of atoms is followed by another in the input sound\n",
    "\n",
    "For further details, see: Cella, Carmine-Emanuele & Burred, Juan José. (2013). *Advanced sound hybridizations by means of the theory of sound-types.* ICMC, 2013.\n",
    "\n",
    "## Section 1: Probabilistic Generation\n",
    "\n",
    "So now that we have seen how the analysis phase of the sound-types works, we will use the Markov chain generated in step 3 to create new audio based on our input signal. The Markov chain is a series of states, where each state is a cluster of atoms. We have already computed the probabilities of transitioning between states. Therefore, we can create a new sound is a similar way to how we created Bach chorales in lab 2:\n",
    "\n",
    "1. Randomly select a starting state\n",
    "2. Select an atom from that state\n",
    "3. Using the transition probabilities of that state, select the next state\n",
    "4. Repeat steps 2 and 3 until a stopping condition is met\n",
    "\n",
    "The atoms that we select in step 2 become our generated audio.\n",
    "\n",
    "The following code cells will walk you through running the analysis and synthesis portions of the sound-types. Run each cell in order and listen to the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T21:49:44.264027Z",
     "start_time": "2023-10-22T21:49:18.212562Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: audioread>=2.1.5 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: numba>=0.45.1 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (0.58.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: pooch>=1.0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from librosa) (23.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from numba>=0.45.1->librosa) (0.41.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: six>=1.3 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from resampy>=0.2.2->librosa) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from scikit-learn>=0.19.1->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot load library '/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib': dlopen(/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib, 0x0002): tried: '/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file), '/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/labs/lib/python3.9/site-packages/soundfile.py:143\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msndfile library not found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m     _snd \u001b[38;5;241m=\u001b[39m \u001b[43m_ffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_libname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: cannot load library '/Users/luke/opt/anaconda3/envs/labs/bin/../lib/libsndfile.dylib': dlopen(/Users/luke/opt/anaconda3/envs/labs/bin/../lib/libsndfile.dylib, 0x0002): Library not loaded: @rpath/libvorbis.0.4.9.dylib\n  Referenced from: <C771606A-1297-3378-A599-7AC299AC99E2> /Users/luke/opt/anaconda3/envs/labs/lib/libsndfile.1.0.31.dylib\n  Reason: tried: '/Users/luke/opt/anaconda3/envs/labs/lib/libvorbis.0.4.9.dylib' (no such file), '/Users/luke/opt/anaconda3/envs/labs/lib/libvorbis.0.4.9.dylib' (no such file), '/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/../../libvorbis.0.4.9.dylib' (no such file), '/Users/luke/opt/anaconda3/envs/labs/bin/../lib/libvorbis.0.4.9.dylib' (no such file), '/usr/local/lib/libvorbis.0.4.9.dylib' (no such file), '/usr/lib/libvorbis.0.4.9.dylib' (no such file, not in dyld cache)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install librosa\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoundfile\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/labs/lib/python3.9/site-packages/soundfile.py:162\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(_path):\n\u001b[1;32m    160\u001b[0m         _path \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 162\u001b[0m     _snd \u001b[38;5;241m=\u001b[39m \u001b[43m_ffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_soundfile_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_libname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m __libsndfile_version__ \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(_snd\u001b[38;5;241m.\u001b[39msf_version_string())\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __libsndfile_version__\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibsndfile-\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mOSError\u001b[0m: cannot load library '/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib': dlopen(/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib, 0x0002): tried: '/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file), '/Users/luke/opt/anaconda3/envs/labs/lib/python3.9/site-packages/_soundfile_data/libsndfile.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from st_tools import *\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "\n",
    "N_COEFF = 14\n",
    "ST_RATIO = .9\n",
    "N_FRAMES = 500\n",
    "FRAME_SIZE = 1024\n",
    "HOP_SIZE = 512\n",
    "MAX_LOOPS = 3 \n",
    "SR = 44100\n",
    "\n",
    "SAMPLES_PATH = Path('./samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to choose a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T21:49:58.460509Z",
     "start_time": "2023-10-22T21:49:58.436397Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(file\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mPath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./samples\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mis_file()]\n\u001b[1;32m      3\u001b[0m sample_dropdown \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mDropdown(\n\u001b[1;32m      4\u001b[0m     options\u001b[38;5;241m=\u001b[39msample_list,\n\u001b[1;32m      5\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a button widget\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "sample_list = [str(file.name) for file in Path('./samples').iterdir() if file.is_file()]\n",
    "\n",
    "sample_dropdown = widgets.Dropdown(\n",
    "    options=sample_list,\n",
    "    description=\"Sample:\"\n",
    ")\n",
    "\n",
    "# Create a button widget\n",
    "button = widgets.Button(description=\"Listen\")\n",
    "\n",
    "# Create an Output widget to display the generated music\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Define a function to be called when the button is clicked\n",
    "def on_button_click(b):\n",
    "    with output_widget:\n",
    "        global y\n",
    "        clear_output(wait=True)  # Clear the output widget without clearing the dropdowns\n",
    "        path = Path('./samples') / sample_dropdown.value\n",
    "        y, _ = librosa.load(path, sr=SR)\n",
    "        display(Audio(y, rate=SR))\n",
    "\n",
    "# Attach the function to the button's click event\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the widgets and button\n",
    "widgets.VBox([sample_dropdown, button, output_widget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following code to generate a new sound based on your chosen sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T21:50:18.274187Z",
     "start_time": "2023-10-22T21:50:16.957590Z"
    }
   },
   "outputs": [],
   "source": [
    "print ('[soundtypes - probabilistic generation]\\n')\n",
    "print ('computing features...')\n",
    "y_pad = np.zeros(len(y) + FRAME_SIZE)\n",
    "y_pad[1:len(y)+1] = y\n",
    "C = librosa.feature.mfcc(y=y, sr=SR, n_mfcc=N_COEFF, n_fft=FRAME_SIZE, \n",
    "                         hop_length=HOP_SIZE)\n",
    "\n",
    "print ('multidimensional scaling...')\n",
    "mds = MDS(2)\n",
    "C_scaled = mds.fit_transform (C.T)\n",
    "\n",
    "print ('computing soundtypes...')\n",
    "(dictionary, markov, centroids, labels) = \\\n",
    "    make_soundtypes(C_scaled, ST_RATIO)\n",
    "n_clusters = centroids.shape[0]\n",
    "\n",
    "# print (markov)\n",
    "print ('generate new sequence...')\n",
    "w1 = np.random.randint (n_clusters)\n",
    "prev_w1 = 0\n",
    "loops = 0\n",
    "gen_sequence = []\n",
    "gen_sound = np.zeros(N_FRAMES * HOP_SIZE + FRAME_SIZE)\n",
    "for i in range(N_FRAMES):\n",
    "    l = markov[(w1)]\n",
    "    if len(l) == 0:\n",
    "        w1 = np.random.randint(n_clusters)\n",
    "    else:\n",
    "        w1 = l[np.random.randint(len(l))]\n",
    "    if prev_w1 == w1:\n",
    "        loops += 1\n",
    "\n",
    "    if loops > MAX_LOOPS:\n",
    "        w1 = np.random.randint(n_clusters)\n",
    "        loops = 0\n",
    "\n",
    "    gen_sequence.append(w1)\n",
    "    p = dictionary[(w1)]\n",
    "    atom = p[np.random.randint(len(p))]\n",
    "\n",
    "    chunk = y_pad[atom*HOP_SIZE:atom*HOP_SIZE+FRAME_SIZE] \\\n",
    "        * np.hanning(FRAME_SIZE)\n",
    "    gen_sound[i*HOP_SIZE:i*HOP_SIZE+FRAME_SIZE] += chunk\n",
    "\n",
    "print ('saving audio data...')\n",
    "# sf.write('generated_sound.wav', gen_sound, SR)\n",
    "\n",
    "print('done.')\n",
    "\n",
    "print(\"Generated audio:\")\n",
    "Audio(gen_sound, rate=SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Probabilistic generation with onsets\n",
    "\n",
    "In this section we again perform probabilistic generation, however the way that we create the atoms is different. In part 1, every atom was of equal length, around 40 ms. Now, we will try to compute when a new *onset* occurs in the input sound, and have each onset be a new atom. For example, if the input is a recording of a piano playing a melody, each new note in the melody is a new onset. The onsets are determined by measuring the *spectral flux*, which measures how quickly the spectrum of the sound is changing. When there is a new onset, for example a note played on the piano, the spectral flux has a high value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T21:50:29.621487Z",
     "start_time": "2023-10-22T21:50:29.614426Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from st_tools import get_segments, make_soundtypes\n",
    "\n",
    "N_COEFF = 20\n",
    "ST_RATIO = .5\n",
    "N_FRAMES = 100\n",
    "FRAME_SIZE = 1024\n",
    "HOP_SIZE = 1024\n",
    "MAX_LOOPS = 3\n",
    "WIDTH = 16\n",
    "FADE_MS = 10\n",
    "SR = 44100\n",
    "\n",
    "SAMPLES_PATH = Path('./samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the previous part, we first set our input sound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T21:50:33.638390Z",
     "start_time": "2023-10-22T21:50:33.609231Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_list = [str(file.name) for file in Path('./samples').iterdir() if file.is_file()]\n",
    "\n",
    "sample_dropdown = widgets.Dropdown(\n",
    "    options=sample_list,\n",
    "    description=\"Sample:\"\n",
    ")\n",
    "\n",
    "# Create a button widget\n",
    "button = widgets.Button(description=\"Listen\")\n",
    "\n",
    "# Create an Output widget to display the generated music\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Define a function to be called when the button is clicked\n",
    "def on_button_click(b):\n",
    "    with output_widget:\n",
    "        global y\n",
    "        clear_output(wait=True)  # Clear the output widget without clearing the dropdowns\n",
    "        path = Path('./samples') / sample_dropdown.value\n",
    "        y, _ = librosa.load(path, sr=SR)\n",
    "        display(Audio(y, rate=SR))\n",
    "\n",
    "# Attach the function to the button's click event\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the widgets and button\n",
    "widgets.VBox([sample_dropdown, button, output_widget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the soundtypes using onsets to create each atom. What differences do you notice? Does one method work \"better\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print ('[soundtypes - probabilistic generation on onsets]\\n')\n",
    "print ('computing segments...')\n",
    "\n",
    "\n",
    "(segments, onsets, flux) = get_segments (y, SR, FRAME_SIZE, HOP_SIZE, \\\n",
    "    FADE_MS, WIDTH)\n",
    "\n",
    "print ('computing features...')\n",
    "features = []\n",
    "for i in range (len(segments)):\n",
    "    C = librosa.feature.mfcc(y=segments[i], sr=SR, n_mfcc=N_COEFF,\n",
    "                             n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
    "    features.append(np.mean (C, axis=1))\n",
    "\n",
    "C = np.vstack(features)\n",
    "\n",
    "print ('multidimensional scaling...')\n",
    "mds = MDS(2)\n",
    "C_scaled = mds.fit_transform (C)\n",
    "\n",
    "print ('computing soundtypes...')\n",
    "(dictionary, markov, centroids, labels) = \\\n",
    "    make_soundtypes(C_scaled, ST_RATIO)\n",
    "n_clusters = centroids.shape[0]\n",
    "\n",
    "print ('generate new sequence...')\n",
    "w1 = np.random.randint (n_clusters)\n",
    "prev_w1 = 0\n",
    "loops = 0\n",
    "gen_sequence = []\n",
    "gen_sound = []\n",
    "for i in range(N_FRAMES):\n",
    "    l = markov[(w1)]\n",
    "    if len(l) == 0:\n",
    "        w1 = np.random.randint(n_clusters)\n",
    "    else:\n",
    "        w1 = l[np.random.randint(len(l))]\n",
    "    if prev_w1 == w1:\n",
    "        loops += 1\n",
    "\n",
    "    if loops > MAX_LOOPS:\n",
    "        w1 = np.random.randint(n_clusters)\n",
    "        loops = 0\n",
    "\n",
    "    gen_sequence.append(w1)\n",
    "    p = dictionary[(w1)]\n",
    "    atom = p[np.random.randint(len(p))]\n",
    "\n",
    "    gen_sound.append (segments[atom])\n",
    "\n",
    "gen_sound = np.hstack (gen_sound)\n",
    "\n",
    "print ('saving audio data...')\n",
    "# sf.write('generated_sound.wav', gen_sound, sr)\n",
    "\n",
    "print('done.')\n",
    "\n",
    "print(\"Generated audio:\")\n",
    "\n",
    "Audio(gen_sound, rate=SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sound Hybridization\n",
    "\n",
    "Another operation possible through the sound-types is sound hybridization. Similar to style transfer, sound hybridization takes the spectral content of one sound and applies it to the temporal aspects of another sound.\n",
    "\n",
    "Here is a description of the process from the paper on sound-types:\n",
    "\n",
    "\"It is possible to subject two different sounds to separate types and rules inferences, and then impose or merge one sound’s types or rules with the others’. The sound-types inferred from a signal (the source) are replaced by, or merged with, the sound-types inferred from a target signal. Each sound-type from the source is matched with a sound-type from the target, in terms of a similarity measure between the centroids of their corresponding feature clusters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from st_tools import make_soundtypes\n",
    "import soundfile as sf\n",
    "\n",
    "N_COEFF = 13\n",
    "FRAME_SIZE = 2048\n",
    "HOP_SIZE = 1024\n",
    "ST_RATIO = .7\n",
    "K = 5\n",
    "SR = 44100\n",
    "\n",
    "SAMPLES_PATH = Path('./samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, set the \"source\" and \"target\" files, then running the cell after to visualize and listen to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Source File\n",
    "\n",
    "samples = list(SAMPLES_PATH.iterdir())\n",
    "samples = [file.name for file in samples]\n",
    "w_src = widgets.Dropdown(options=samples, value='cage.wav', description='Source File:')\n",
    "source_file = SAMPLES_PATH / w_src.value\n",
    "y_src = None\n",
    "sr = None\n",
    "\n",
    "y_src, sr = librosa.load(source_file, sr=SR)\n",
    "\n",
    "def on_change_src(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global source_file\n",
    "        global y_src\n",
    "        global sr\n",
    "        \n",
    "        source_file = SAMPLES_PATH / change.new\n",
    "        y_src, sr = librosa.load(source_file, sr=SR)\n",
    "        print(\"changed source file to\", source_file)\n",
    "        \n",
    "w_src.observe(on_change_src)\n",
    "\n",
    "# Target File\n",
    "\n",
    "w_dst = widgets.Dropdown(options=samples, value='lachenmann.wav', description='Target File:')\n",
    "target_file = SAMPLES_PATH / w_dst.value\n",
    "y_dst = None\n",
    "sr = None\n",
    "\n",
    "y_dst, sr = librosa.load(target_file, sr=SR)\n",
    "\n",
    "def on_change_dst(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global target_file\n",
    "        global y_dst\n",
    "        \n",
    "        target_file = SAMPLES_PATH / change.new\n",
    "        y_dst, _ = librosa.load(target_file, sr=SR)\n",
    "        print(\"changed destination file to\", target_file)\n",
    "        \n",
    "w_dst.observe(on_change_dst)\n",
    "\n",
    "display(w_src, w_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source audio:\", source_file)\n",
    "display(Audio(y_src, rate=SR))\n",
    "\n",
    "print(\"Target audio:\", target_file)\n",
    "display(Audio(y_dst, rate=SR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting your source and target files, run the following code to generate a new sample that is a hybridization of the two sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print ('[soundtypes - timbre matching]\\n')\n",
    "print ('computing features...')\n",
    "\n",
    "y_pad_src = np.zeros(len(y_src) + FRAME_SIZE)\n",
    "y_pad_src[1:len(y_src)+1] = y_src\n",
    "\n",
    "C_src = librosa.feature.mfcc(y=y_src, sr=SR, n_mfcc=N_COEFF,\n",
    "                                 n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
    "\n",
    "y_pad_dst = np.zeros(len(y_dst) + FRAME_SIZE)\n",
    "y_pad_dst[1:len(y_dst)+1] = y_dst\n",
    "\n",
    "C_dst = librosa.feature.mfcc(y=y_dst, sr=SR, n_mfcc=N_COEFF,\n",
    "                                 n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
    "\n",
    "C_scaled_dst = C_dst.T\n",
    "C_scaled_src = C_src.T\n",
    "\n",
    "scaler = StandardScaler ()\n",
    "C_scaled_dst = scaler.fit_transform (C_scaled_dst)\n",
    "C_scaled_src = scaler.fit_transform (C_scaled_src)\n",
    "\n",
    "print ('computing soundtypes...')\n",
    "(dictionary_src, markov_src, centroids_src, labels_src) = \\\n",
    "    make_soundtypes(C_scaled_src, ST_RATIO)\n",
    "n_clusters_src = centroids_src.shape[0]\n",
    "(dictionary_dst, markov_dst, centroids_dst, labels_dst) = \\\n",
    "    make_soundtypes(C_scaled_dst, ST_RATIO)\n",
    "n_clusters_dst = centroids_dst.shape[0]\n",
    "\n",
    "print ('matching clusters...')\n",
    "knn = NearestNeighbors(n_neighbors=K).fit(centroids_dst)\n",
    "dist, idxs = knn.kneighbors(centroids_src)\n",
    "\n",
    "print ('generate hybridization...')\n",
    "n_frames = len(labels_src)\n",
    "gen_sound = np.zeros(n_frames * HOP_SIZE + FRAME_SIZE)\n",
    "for i in range(n_frames):\n",
    "    labels_match = idxs[labels_src[i], :]\n",
    "    x = labels_match[np.random.randint(K)]\n",
    "    p = dictionary_dst[x]\n",
    "    if len(p) == 0:\n",
    "        atom = 0\n",
    "    else:\n",
    "        atom = p[np.random.randint(len(p))]\n",
    "\n",
    "    amp = np.sum (np.abs(y_pad_src[i * HOP_SIZE : i * HOP_SIZE + \\\n",
    "        FRAME_SIZE]))\n",
    "    chunk = y_pad_dst[atom * HOP_SIZE : atom * HOP_SIZE + FRAME_SIZE] \\\n",
    "        * np.hanning(FRAME_SIZE)\n",
    "\n",
    "    norm = np.max (np.abs(chunk))\n",
    "    if norm == 0:\n",
    "        norm = 1\n",
    "\n",
    "    chunk /= norm\n",
    "    gen_sound[i * HOP_SIZE : i * HOP_SIZE + FRAME_SIZE] += (chunk * amp / n_frames)\n",
    "\n",
    "print ('saving audio data...')\n",
    "# sf.write('generated_sound.wav', gen_sound, sr)\n",
    "print('done.')\n",
    "\n",
    "print(\"Generated audio:\")\n",
    "Audio(gen_sound, rate=SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about this sound hybridization process? Do you consider it successful at style transfer? Why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
